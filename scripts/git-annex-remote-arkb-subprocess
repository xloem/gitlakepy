#!/usr/bin/env python3

from annexremote import Master as Main, SpecialRemote, RemoteError, UnsupportedRequest

import json, os, random, shutil, subprocess, sys, time
import re, requests

ansi_escape = re.compile(r'\x1B(?:[@-Z\\-_]|\[[0-?]*[ -/]*[@-~])')

TXID_LEN = 43
URI_PROTO = 'arkb-subprocess://'

class ArkbStorageRemote(SpecialRemote):
    def __init__(self, annex):
        super().__init__(annex)
        self.configs = {
            'wallet': 'Path or jwk wallet',
            'bundler': 'ANS-104 bundler node url',
            'gateway': 'Arweave gateway url',
            'combine-to-bytes': 'Consolidate uploads up to this many bytes',
            'subchunk-bytes': 'Break uploads into subfiles of this many bytes',
        }
        self.local_dir = None
        self.closed = False

    def initremote(self):
		# initialize in repo, e.g. create folders or change settings
        # 'git annex initremote' / 'git annex enableremote'

        wallet = self.annex.getconfig('wallet')
        #token = self.annex.getcreds('token')['password']
        if wallet == '':
            raise RemoteError('wallet is required')
        try:
            with open(wallet, 'rt') as wf:
                wallet = wf.read()
            self.annex.setconfig('wallet', wallet.replace('\n', ' '))
        except:
            pass

        #self.w3('token', input=token+'\n')
        self.prepare()


    def prepare(self):
        # prepare to be used for transfers, e.g. open connection
        self.local_dir = os.path.join(self.annex.getgitdir(), self.__class__.__name__, self.annex.getuuid())
        os.makedirs(self.local_dir, exist_ok=True)
        self.wallet_path = os.path.join(self.local_dir, 'wallet.json')
        try:
            with open(self.wallet_path, 'rt') as wallet_file:
                wallet = wallet_file.read()
            assert wallet == self.annex.getconfig('wallet')
        except:
            tmp_wallet_path = self.wallet_path + '.tmp'
            with open(tmp_wallet_path, 'wt') as wallet_file:
                wallet_file.write(self.annex.getconfig('wallet'))
            os.chmod(tmp_wallet_path, 0o400)
            os.rename(tmp_wallet_path, self.wallet_path)

        self._info('arkb ' + self.arkb('version').strip())

        self.params_deploy = ('--wallet', self.wallet_path, '--no-colors')
        self.bundler = self.annex.getconfig('bundler')
        if self.bundler:
            self.params_deploy = (*self.params_deploy, '--use-bundler', self.bundler)
        self.gateway = self.annex.getconfig('gateway')
        if not self.gateway:
            self.gateway = 'https://arweave.net'
        else:
            self.params_deploy = (*self.params_deploy,'--gateway', self.gateway)
        self.combine_to = self.annex.getconfig('combine-to-bytes')
        if self.combine_to == '' and self.bundler:
            self.combine_to = 100000 * 32
        self.combining = {}
        self.combined = 0
        if self.combine_to:
            self.combine_to = int(self.combine_to)
        self.subchunk = self.annex.getconfig('subchunk-bytes')
        if self.subchunk != '':
            self.subchunk = int(self.subchunk)
        if self.combine_to or self.subchunk:
            if self.combine_to and self.subchunk and self.subchunk > self.combine_to:
                raise RemoteError('unimplemented: subchunk-bytes > combine-to-bytes')
                
            self.combining_dir = os.path.join(self.local_dir, str(os.getpid()))
            os.makedirs(self.combining_dir, exist_ok = True)

    def do_combine(self, tags_dict = {}, **tags):
        if len(self.combining) == 0:
            return
        index_filename = 'index.txt'
        with open(os.path.join(self.combining_dir, index_filename), 'wt') as index_file:
            index_file.write('\n'.join(self.combining.keys()))
        if tags:
            tags_dict = tags_dict.copy()
            tags_dict.update(tags)
        keytagparams = []
        for name, value in tags_dict.items():
            keytagparams.extend((
                '--tag-name', name,
                '--tag-value', value
            ))
        output = self.arkb(
            'deploy',
            self.combining_dir,
            *keytagparams,
            '--concurrency', str(5 if len(self.combining) < 5 else len(self.combining)),
            '--auto-confirm',
            '--index', index_filename,
            *self.params_deploy
        )
        if 'Failed to deploy' in output:
            raise RemoteError(output.replace('\n', ' '))
        manifest_filename = os.path.join(self.combining_dir, 'manifest.arkb')
        while not os.path.exists(manifest_filename):
            for path in self.combining.keys():
                manifest_filename = os.path.join(self.combining_dir, os.path.basename(path), 'manifest.arkb')
        if not os.path.exists(manifest_filename):
            self.combined = 0 # to quickly prevent cleanup during debugging
            raise RemoteError(f'not found after deploy: {manifest_filename}')
        lines = output.split('\n')
        self._debug('lines[-3] = ' + repr(lines[-3]))
        self._debug('lines[-2] = ' + repr(lines[-2]))
        self._debug('lines[-1] = ' + repr(lines[-1]))
        manifest_txid = lines[-2].split('/')[-1]
        with open(manifest_filename) as manifest_file:
            manifest = json.load(manifest_file)
        key_urls = []
        file_txids = {}
        for key, props in manifest['paths'].items():
            txid = props['id']
            file_txids[key] = txid
            self._debug(f'{key}: {self.gateway}/{txid}')
            if self.combining.get(key, False):
                for url in (*self.txid_urls(txid), *self.txid_uris(txid)):
                    key_urls.append((key, url))
        if self.closed:
            key_urls = '\n'.join((' '.join(keyurl) for keyurl in key_urls))
            result = subprocess.run(
                ('git', 'annex', 'registerurl', '--batch'),
                input=keyurls,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                check=True,
                text=True
            )
            for line in result.stdout:
                self._debug(line)
            for line in result.stderr:
                self._info(line)
        else:
            for key, url in key_urls:
                self.annex.seturlpresent(key, url)
        shutil.rmtree(self.combining_dir)
        os.makedirs(self.combining_dir)
        self.combined = 0
        self.combining = {}
        return manifest_txid, file_txids

    def finish(self):
        self.closed = True
        self.tty = open('/dev/tty', 'wt')
        if (hasattr(self, 'combine_to') and self.combine_to):
            if self.combined and any(self.combining.values()):
                self.do_combine()
        if (hasattr(self, 'combine_to') and self.combine_to) or (hasattr(self, 'subchunk') and self.subchunk):
            shutil.rmtree(self.combining_dir)
    def _info(self, txt):
        if self.closed:
            self.tty.write(f'Pre-shutdown: {txt}\n')
        else:
            self.annex.info(txt)
    def _debug(self, txt):
        if self.closed:
            return self._info(txt)
        else:
            self.annex.debug(txt)

    def transfer_store(self, key, filename):
        with open(filename, 'rb') as file:
            file.seek(0, os.SEEK_END)
            size = file.tell()
        if self.combine_to and size < self.combine_to:
            if self.combined + size > self.combine_to:
                self.do_combine()
            newfilename = key #'c-' + str(len(self.combining))
            newfullpath = os.path.join(self.combining_dir, newfilename)
            os.rename(filename, newfullpath)
            self.combining[key] = True#size#newfullpath#(newfilename, keytagparams)
            self.combined += size
        elif self.subchunk and size > self.subchunk:
            new_subdir = key
            new_fulldir = os.path.join(self.combining_dir, key); os.makedirs(new_fulldir, exist_ok=True)
            progress = 0
            with open(filename, 'rb') as file:
                keytxids = {}
                for offset in range(0, size, self.subchunk):
                    newfilename = str(offset)
                    data = file.read(self.subchunk)
                    if self.combined + len(data) > self.combine_to:
                        manifest_txid, file_txids = self.do_combine()
                        keytxids.update(file_txids)
                        self.annex.progress(progress)
                        new_fulldir = os.path.join(self.combining_dir, key); os.makedirs(new_fulldir, exist_ok=True)
                    progress += len(data)
                    self.combined += len(data)
                    self.combining[new_subdir + '/' + newfilename] = False
                    with open(os.path.join(new_fulldir, newfilename), 'wb') as newfile:
                        newfile.write(data)
            keytxids.update(self.do_combine()[1])
            txids = []
            self._debug('keytxids: ' + repr(keytxids))
            keytxids = [(*key.split('/',1), txid) for key, txid in keytxids.items() if '/' in key]
            keytxids.sort(key=lambda item: int(item[1]))
            txids = [txid for txkey, offset, txid in keytxids if txkey == key]
            assert len(txids) > 0
            
            # so we'll probably have multiple index files
            # in case there are many subchunks.
            file_idx = 0
            idx_file = None
            for txid in txids:
                if idx_file is None or idx_file_size + len(txid) + 1 > self.subchunk:
                    if idx_file is not None:
                        self.combined += idx_file_size
                        idx_file.close()
                    file_idx += 1
                    idx_file = open(os.path.join(self.combining_dir, str(file_idx)), 'wt')
                    self.combining[str(file_idx)] = False
                    idx_file.write(txid)
                    idx_file_size = len(txid)
                else:
                    idx_file.write('\n' + txid)
                    idx_file_size += 1 + len(txid)
            self.combined += idx_file_size
            idx_file.close()

            manifest_txid, file_txids = self.do_combine({
                'application': 'git-annex-remote-arkb-subprocess',
                'format': 'subchunk-txid-files',
                'git-annex-key': key
            })

            self.annex.seturipresent(key, URI_PROTO + manifest_txid)

            self.annex.progress(progress)
        else:
            keybackend, keyname = key.split('--')
            keybackend, *keyparts = keybackend.split('-')
            keyparts = {
                keypart[0]: keypart[1:]
                for keypart in keyparts
            }
            keyparts['backend'] = keybackend
            keyparts['name'] = keyname
            keytagparams = []
            for name, value in keyparts.items():
                keytagparams.extend((
                    '--tag-name', 'git-annex-key-' + name,
                    '--tag-value', value
                ))
            keytagparams.extend((
                    '--tag-name', 'IPFS-Add',
                    '--tag-value', key,
                    '--tag-name', 'git-annex-key',
                    '--tag-value', key,
            ))
            output = self.arkb(
                    'deploy',
                    filename,
                    *keytagparams,
                    '--auto-confirm',
                    *self.params_deploy
            )
            if 'Failed to deploy' in output:
                raise RemoteError(output.replace('\n', ' '))
            lines = output.split('\n')
            self._debug('lines[-3] = ' + repr(lines[-3]))
            self._debug('lines[-2] = ' + repr(lines[-2]))
            self._debug('lines[-1] = ' + repr(lines[-1]))
            txid = lines[-2].split('/')[-1]
            for url in self.txid_urls(txid):
                self.annex.seturlpresent(key, url)
            for uri in self.txid_uris(txid):
                self.annex.seturipresent(key, uri)

    def transfer_retrieve(self, key, local_file):
        if self.combine_to and key in self.combining:
            self.do_combine()
        last_exc = None
        for txid in self.txids(key):
            for url in self.txid_urls(txid):
                try:
                    self.retrieve_url(url, local_file)
                    return
                except Exception as exc:
                    last_exc = exc
                    continue
            for url in self.txid_tmpurls(txid):
                try:
                    self.retrieve_url(url, local_file)
                    self._info(f'{key} stalled at {url}')
                    return
                except Exception as exc:
                    last_exc = exc
                    continue
        raise last_exc

    def retrieve_url(self, url, local_file):
        self._debug(f'retrieve_url({repr(url)}, {repr(local_file)}')
        urlparts = url.split('/')
        for idx, part in enumerate(urlparts):
            if len(part) == TXID_LEN:
                txidurlidx = idx
                break
        self._debug(f'txid = {urlparts[txidurlidx]}')

        size = 0

        with requests.get(url, stream=True) as stream:
            stream.raise_for_status()
            # manifest from bundler node indicating folder
            # or '/' trailing url from gateway indicating folder
            # assume subchunk format, files containing txids to be concatenated
            self._debug(f'Content-Type = {stream.headers["Content-Type"]}')
            self._debug(f'url = {stream.url}')
            if stream.headers['Content-Type'] == 'application/x.arweave-manifest+json' or stream.url[-1] == '/':
                if stream.headers['Content-Type'] == 'application/x.arweave-manifest+json':
                    manifest = stream.json()
                    stream.close()
                    assert manifest['manifest'] == 'arweave/paths'
                    assert manifest['version'] == '0.1.0'
                    filetxids = {key: value['id'] for key, value in manifest['paths']}
                    index_file = manifest['index']
                    urlparts[txidurlidx] = filetxids[index_file]
                    stream = requests.get('/'.join(urlparts))
                    stream.raise_for_status()
                    subfiles = {}
                    for subfile in stream.text.split('\n'):
                        urlparts[txidurlidx] = filetxids[subfile]
                        subfiles[subfile] = '/'.join(urlparts)
                else:
                    subfiles = {}
                    subfilenames = stream.text.split('\n')
                    stream.close()
                    for subfile in subfilenames:
                        subfiles[subfile] = url + '/' + subfile
                with open(local_file, 'wb') as file:
                    for url in subfiles.values():
                        self._debug(url)
                        response = requests.get(url)
                        response.raise_for_status()
                        for txid in response.text.split('\n'):
                            urlparts[txidurlidx] = txid
                            url = '/'.join(urlparts)
                            self._debug(url)
                            with requests.get(url, stream=True) as stream:
                                for chunk in stream.iter_content(chunk_size=65536):
                                    file.write(chunk)
                                    size += len(chunk)
                                    self.annex.progress(size)
            else:
                with open(local_file, 'wb') as file:
                    for chunk in stream.iter_content(chunk_size=65536):
                        file.write(chunk)
                        size += len(chunk)
                        self.annex.progress(size)

    def remove(self, key):
        if self.combine_to:
            self.combined -= self.combining.pop(key, 0)

        for txid in self.txids(key):
            for url in self.txid_urls(txid):
                self.annex.seturlmissing(key, url)
            for uri in self.txid_uris(txid):
                self.annex.seturimissing(key, uri)
            self.annex.seturimissing(key, URI_PROTO + txid)

    def txids(self, key):
        return [url.split('/',3)[-1] for url in (*self.annex.geturls(key, self.gateway + '/'), *self.annex.geturls(key, URI_PROTO))]
    def checkpresent(self, key):
        if self.combine_to and key in self.combining:
            return True
        for txid in self.txids(key):
            for url in (*self.txid_urls(txid), URI_PROTO + txid):
                try:
                    requests.head(url).raise_for_status()
                    return True
                except:
                    continue
            for url in self.txid_tmpurls(txid):
                try:
                    requests.head(url).raise_for_status()
                    self._info(f'{key} stalled at {url}')
                    return True
                except:
                    continue
        return False

    def getavailability(self):
        return 'global'

    def txid_tmpurls(self, txid):
        if self.bundler:
            return [
                f'{self.bundler}/tx/{txid}/data',
            ]
        else:
            return []


    def txid_urls(self, txid):
        return [
            f'{self.gateway}/{txid}',
        ]

    def txid_uris(self, txid):
        return [
        ]

    #def whereis(self, key):
    #    item = self.stored.get(key)
    #    if item is None:
    #        return ''
    #    else:
    #        cid = item['cid']
    #        return ' '.join((self.cid_uris(cid)))

    def arkb(self, *params, input=None, backoff_secs=8, retries=10):
        try:
            self._debug('arkb ' + ' '.join(params))
            result = subprocess.run(
                (os.environ.get('ARKB', 'arkb'), *params),
                env={'PATH':os.environ.get('PATH', ''), 'HOME':self.local_dir},
                input=input,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                check=True,
                text=True
            )
            stdout = ansi_escape.sub('', result.stdout)
            stderr = ansi_escape.sub('', result.stderr)
            for line in stdout.split('\n'):
                self._debug(line[:80])
            for line in stderr.split('\n'):
                self._debug(line[:80])
            self._debug('PROCESS SUCCESS arkb ' + ' '.join(params))
        except subprocess.CalledProcessError as err:
            #self.confirmed = 0 # don't clean up state
            self._info('err: arkb ' + ' '.join(params) + repr(err))
            stdout = ansi_escape.sub('', err.stdout)
            stderr = ansi_escape.sub('', err.stderr)
            for line in stdout.split('\n'):
                self._info(line[:80])
            for line in stderr.split('\n'):
                self._info(line[:80])
            if retries:
                return self.arkb(*params, input=input, backoff_secs=backoff_secs, retries=retries-1)
            #if 'JSON.parse' in err.stderr or 'Too Many Requests' in err.stdout:
            #    delay = backoff_secs + backoff_secs * random.random()
            #    self._info(f'Waiting {int(delay+0.5)} seconds.')
            #    for line in err.stdout.split('\n'):
            #        self._info(line)
            #    for line in err.stderr.split('\n'):
            #        self._info(line)
            #    time.sleep(delay)
            #    self._info(f'Done waiting for {int(delay+0.5)} seconds.')
            #    return self.w3(*params, input=input, backoff_secs = delay)
            #elif 'EAI_AGAIN' in err.stderr:
            #    self._info(f'EAI_AGAIN')
            #    return self.w3(*params, input=input, backoff_secs = backoff_secs)

            raise RemoteError((err.stdout + ' '  + err.stderr).replace('\n', ' '))
        return stdout
    #def setup(self):
    #    subprocess.run(('w3', 

    #def keystatuses(self, key):
    #    item = self.stored.get(key, dict(pin=[], deal=[]))
    #    result = set((*(pin['status'] for pin in item.get('pins',())), *(deal['status'] for deal in item.get('deals',()))))
    #    if '_git_annex_just_put' in item:
    #        result.add('JustPut')
    #    return result

if __name__ == '__main__':
    main = Main()
    remote = ArkbStorageRemote(main)
    main.LinkRemote(remote)
    try:
        main.Listen()
    finally:
        remote.finish()
