#!/usr/bin/env python3

from annexremote import Master as Main, SpecialRemote, RemoteError, UnsupportedRequest

import concurrent.futures
import json, os, random, shutil, subprocess, sys, time
import re, requests

ansi_escape = re.compile(r'\x1B(?:[@-Z\\-_]|\[[0-?]*[ -/]*[@-~])')

TXID_LEN = 43
URI_PROTO = 'arkb-subprocess://'

class ArkbStorageRemote(SpecialRemote):
    def __init__(self, annex):
        super().__init__(annex)
        self.configs = {
            'wallet': 'Path or jwk wallet',
            'bundler': 'ANS-104 bundler node url',
            'gateway': 'Arweave gateway url',
            'combine-to-bytes': 'Consolidate uploads up to this many bytes',
            'subchunk-bytes': 'Break uploads into subfiles of this many bytes',
            'timeout': 'Network timeout in seconds',
        }
        self.local_dir = None
        self.closed = False

    def initremote(self):
		# initialize in repo, e.g. create folders or change settings
        # 'git annex initremote' / 'git annex enableremote'

        wallet = self.annex.getconfig('wallet')
        #token = self.annex.getcreds('token')['password']
        if wallet == '':
            self._info('Warning: no wallet specified. Only subsidised uploads will work.')
        else:
            try:
                with open(wallet, 'rt') as wf:
                    wallet = wf.read()
                self.annex.setconfig('wallet', wallet.replace('\n', ' '))
            except:
                pass

        #self.w3('token', input=token+'\n')
        self.prepare()


    def prepare(self):
        # prepare to be used for transfers, e.g. open connection
        self.uuid = self.annex.getuuid()
        self.local_dir = os.path.join(self.annex.getgitdir(), self.__class__.__name__, self.uuid)
        os.makedirs(self.local_dir, exist_ok=True)
        if self.annex.getconfig('wallet') != '':
            self.wallet_path = os.path.join(self.local_dir, 'wallet.json')
            try:
                with open(self.wallet_path, 'rt') as wallet_file:
                    wallet = wallet_file.read()
                assert wallet == self.annex.getconfig('wallet')
            except:
                tmp_wallet_path = self.wallet_path + '.tmp'
                with open(tmp_wallet_path, 'wt') as wallet_file:
                    wallet_file.write(self.annex.getconfig('wallet'))
                os.chmod(tmp_wallet_path, 0o400)
                os.rename(tmp_wallet_path, self.wallet_path)
            self.params_deploy = ('--wallet', self.wallet_path, '--no-colors')
        else:
            self.wallet_path = ''
            self.params_deploy = ('--no-colors',)

        self._info('arkb ' + self.arkb('version').strip())

        self.bundler = self.annex.getconfig('bundler')
        if self.bundler:
            self.params_deploy = (*self.params_deploy, '--use-bundler', self.bundler)
        self.gateway = self.annex.getconfig('gateway')
        if not self.gateway:
            self.gateway = 'https://arweave.net'
        else:
            self.params_deploy = (*self.params_deploy,'--gateway', self.gateway)
        self.timeout = self.annex.getconfig('timeout')
        if not self.timeout:
            self.timeout = str(2147483646 / 1000)
        self.params_deploy = (*self.params_deploy,'--timeout', str(int(float(self.timeout) * 1000)))

        self.combine_to = self.annex.getconfig('combine-to-bytes')
        if self.combine_to == '' and self.bundler:
            self.combine_to = 100000 * 32
        self.combining = {}
        self.combined = 0
        self.combined_pathnames = 0
        if self.combine_to:
            self.combine_to = int(self.combine_to)
        self.subchunk = self.annex.getconfig('subchunk-bytes')
        if self.subchunk != '':
            self.subchunk = int(self.subchunk)
        self.urlqueue_dir = os.path.join(self.local_dir, 'urlqueue')
        try:
            for file in os.listdir(self.urlqueue_dir):
                with open(os.path.join(self.urlqueue_dir, file), 'rt') as fh:
                    for line in fh:
                        key, url = line.rstrip().split(' ', 1)
                        self.annex.seturlpresent(key, url)
                os.unlink(file)
        except FileNotFoundError:
            pass
        if self.combine_to or self.subchunk:
            if self.combine_to and self.subchunk and self.subchunk > self.combine_to:
                raise RemoteError('unimplemented: subchunk-bytes > combine-to-bytes')
                
            self.combining_dir = os.path.join(self.local_dir, str(os.getpid()))
            os.makedirs(self.combining_dir, exist_ok = True)

    def do_combine(self, tags_dict = {}, **tags):
        if len(self.combining) == 0:
            return
        index_filename = 'index.txt'
        with open(os.path.join(self.combining_dir, index_filename), 'wt') as index_file:
            index_file.write('\n'.join(self.combining.keys()))
        if tags:
            tags_dict = tags_dict.copy()
            tags_dict.update(tags)
        keytagparams = []
        for name, value in tags_dict.items():
            keytagparams.extend((
                '--tag-name', name,
                '--tag-value', value
            ))
        output = self.arkb(
            'deploy',
            self.combining_dir,
            *keytagparams,
            '--concurrency', str(5 if len(self.combining) < 5 else len(self.combining)),
            '--auto-confirm',
            '--index', index_filename,
            *self.params_deploy
        )
        manifest_filename = os.path.join(self.combining_dir, 'manifest.arkb')
        if not os.path.exists(manifest_filename):
            for path in self.combining.keys():
                manifest_filename = os.path.join(self.combining_dir, os.path.dirname(path), 'manifest.arkb')
                if os.path.exists(manifest_filename):
                    break
        if not os.path.exists(manifest_filename):
            self.combined = 0 # to quickly prevent cleanup during debugging
            self.combined_pathnames = 0
            self.combining_dir = self.combining_dir + '_'
            raise RemoteError(f'not found on fs after arkb deploy: {manifest_filename}')
        lines = output.split('\n')
        self._debug('lines[-3] = ' + repr(lines[-3]))
        self._debug('lines[-2] = ' + repr(lines[-2]))
        self._debug('lines[-1] = ' + repr(lines[-1]))
        manifest_txid = lines[-2].split('/')[-1]
        with open(manifest_filename) as manifest_file:
            manifest = json.load(manifest_file)
        key_urls = []
        file_txids = {}
        for key, props in manifest['paths'].items():
            txid = props['id']
            file_txids[key] = txid
            self._debug(f'{key}: {self.gateway}/{txid}')
            if self.combining.get(key, False):
                for url in (*self.txid_urls(txid), *self.txid_uris(txid)):
                    key_urls.append((key, url))
        if self.closed:
            example_key_url = key_urls[0][0]
            key_urls = '\n'.join((' '.join(keyurl) for keyurl in key_urls))
            urlqueue_fn = str(os.getpid()) + '.urls'
            with open(os.path.join(self.local_dir, urlqueue_fn), 'wt') as fh:
                fh.write(key_urls)
            os.rename(os.path.join(self.local_dir, urlqueue_fn), os.path.join(self.urlqueue_dir, urlqueue_fn))
            result = subprocess.run(
                ('git', 'annex', '--debug', 'checkpresentkey', example_key_url, self.uuid),
                input=keyurls,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                check=True,
                text=True
            )
            for line in result.stdout:
                self._debug(line)
            for line in result.stderr:
                self._info(line)
        else:
            for key, url in key_urls:
                self.annex.seturlpresent(key, url)
        shutil.rmtree(self.combining_dir)
        os.makedirs(self.combining_dir)
        self.combined = 0
        self.combined_pathnames = 0
        self.combining = {}
        return manifest_txid, file_txids

    def finish(self):
        self.closed = True
        self.tty = open('/dev/tty', 'wt')
        if (hasattr(self, 'combine_to') and self.combine_to):
            if self.combined and any(self.combining.values()):
                self.do_combine()
        if (hasattr(self, 'combine_to') and self.combine_to) or (hasattr(self, 'subchunk') and self.subchunk):
            shutil.rmtree(self.combining_dir)
    def _info(self, txt):
        if self.closed:
            self.tty.write(f'Pre-shutdown: {txt}\n')
        else:
            self.annex.info(txt)
    def _debug(self, txt):
        if self.closed:
            return self._info(txt)
        else:
            self.annex.debug(txt)

    def transfer_store(self, key, filename):
        with open(filename, 'rb') as file:
            file.seek(0, os.SEEK_END)
            size = file.tell()
        if self.subchunk and size > self.subchunk:
            new_subdir = key
            new_fulldir = os.path.join(self.combining_dir, new_subdir); os.makedirs(new_fulldir, exist_ok=True)
            progress = 0
            with open(filename, 'rb') as file:
                keytxids = {}
                for offset in range(0, size, self.subchunk):
                    newfilename = str(offset)
                    data = file.read(self.subchunk)
                    if self.combined + len(data) > self.combine_to or (self.combined_pathnames + len(newfilename)) * 56 + 77 + len('index.txt') > self.subchunk:
                        manifest_txid, file_txids = self.do_combine()
                        keytxids.update(file_txids)
                        self.annex.progress(progress)
                        new_fulldir = os.path.join(self.combining_dir, new_subdir); os.makedirs(new_fulldir, exist_ok=True)
                    progress += len(data)
                    self.combined += len(data)
                    self.combined_pathnames += len(os.path.join(new_subdir, newfilename))
                    self.combining[new_subdir + '/' + newfilename] = False
                    with open(os.path.join(new_fulldir, newfilename), 'wb') as newfile:
                        newfile.write(data)
            keytxids.update(self.do_combine()[1])
            txids = []
            self._debug('keytxids: ' + repr(keytxids))
            keytxids = [(*key.split('/',1), txid) for key, txid in keytxids.items() if '/' in key]
            keytxids.sort(key=lambda item: int(item[1]))
            txids = [txid for txkey, offset, txid in keytxids if txkey == key]
            assert len(txids) > 0
            
            # so we'll probably have multiple index files
            # in case there are many subchunks.
            file_idx = 0
            idx_file = None
            for txid in txids:
                if idx_file is None or idx_file_size + len(txid) + 1 > self.subchunk:
                    if idx_file is not None:
                        self.combined += idx_file_size
                        idx_file.close()
                    file_idx += 1
                    idx_file = open(os.path.join(self.combining_dir, str(file_idx)), 'wt')
                    self.combined_pathnames += len(str(file_idx))
                    self.combining[str(file_idx)] = False
                    idx_file.write(txid)
                    idx_file_size = len(txid)
                else:
                    idx_file.write('\n' + txid)
                    idx_file_size += 1 + len(txid)
            self.combined += idx_file_size
            idx_file.close()

            manifest_txid, file_txids = self.do_combine({
                'application': 'git-annex-remote-arkb-subprocess',
                'format': 'subchunk-txid-files',
                'git-annex-key': key
            })

            self.annex.seturipresent(key, URI_PROTO + manifest_txid)

            self.annex.progress(progress)
        elif self.combine_to and size < self.combine_to:
            if self.combined + size > self.combine_to:
                self.do_combine()
            newfilename = key #'c-' + str(len(self.combining))
            newfullpath = os.path.join(self.combining_dir, newfilename)
            try:
                os.rename(filename, newfullpath)
            except:
                shutil.copyfile(filename, newfullpath)
            self.combining[key] = True#size#newfullpath#(newfilename, keytagparams)
            self.combined += size
            self.combined_pathnames += len(newfilename)
        else:
            keybackend, keyname = key.split('--')
            keybackend, *keyparts = keybackend.split('-')
            keyparts = {
                keypart[0]: keypart[1:]
                for keypart in keyparts
            }
            keyparts['backend'] = keybackend
            keyparts['name'] = keyname
            keytagparams = []
            for name, value in keyparts.items():
                keytagparams.extend((
                    '--tag-name', 'git-annex-key-' + name,
                    '--tag-value', value
                ))
            keytagparams.extend((
                    '--tag-name', 'IPFS-Add',
                    '--tag-value', key,
                    '--tag-name', 'git-annex-key',
                    '--tag-value', key,
            ))
            output = self.arkb(
                    'deploy',
                    filename,
                    *keytagparams,
                    '--auto-confirm',
                    *self.params_deploy
            )
            if 'Failed to deploy' in output:
                raise RemoteError(output.replace('\n', ' '))
            lines = output.split('\n')
            self._debug('lines[-3] = ' + repr(lines[-3]))
            self._debug('lines[-2] = ' + repr(lines[-2]))
            self._debug('lines[-1] = ' + repr(lines[-1]))
            txid = lines[-2].split('/')[-1]
            for url in self.txid_urls(txid):
                self.annex.seturlpresent(key, url)
            for uri in self.txid_uris(txid):
                self.annex.seturipresent(key, uri)

    def transfer_retrieve(self, key, local_file):
        if self.combine_to and key in self.combining:
            self.do_combine()
        last_exc = RemoteError('Logic error? No urls stored for key ' + key)
        for txid in self.txids(key):
            for url in self.txid_urls(txid):
                try:
                    self.retrieve_url(url, local_file)
                    return
                except Exception as exc:
                    last_exc = exc
                    continue
            for url in self.txid_tmpurls(txid):
                try:
                    self.retrieve_url(url, local_file)
                    self._info(f'{key} or a subpart stalled at {url}')
                    self._info(f'{key} uploaded at {self.last_key_date(key)}')
                    return
                except Exception as exc:
                    last_exc = exc
                    continue
        self._info('retrieve_url: ' + str(last_exc))
        raise RemoteError('retrieve_url: ' + str(last_exc))

    def retrieve_url(self, url, local_file):
        self._debug(f'retrieve_url({repr(url)}, {repr(local_file)}')
        urlparts = url.split('/')
        for idx, part in enumerate(urlparts):
            if len(part) == TXID_LEN:
                txidurlidx = idx
                break
        self._debug(f'txid = {urlparts[txidurlidx]}')

        size = 0

        with requests.get(url, stream=True) as stream:
            stream.raise_for_status()
            # manifest from bundler node indicating folder
            # or '/' trailing url from gateway indicating folder
            # assume subchunk format, files containing txids to be concatenated
            self._debug(f'Content-Type = {stream.headers["Content-Type"]}')
            self._debug(f'url = {stream.url}')
            if stream.headers['Content-Type'] == 'application/x.arweave-manifest+json' or stream.url[-1] == '/':
                if stream.headers['Content-Type'] == 'application/x.arweave-manifest+json':
                    manifest = stream.json()
                    stream.close()
                    assert manifest['manifest'] == 'arweave/paths'
                    assert manifest['version'] == '0.1.0'
                    filetxids = {key: value['id'] for key, value in manifest['paths']}
                    index_file = manifest['index']
                    urlparts[txidurlidx] = filetxids[index_file]
                    stream = requests.get('/'.join(urlparts))
                    stream.raise_for_status()
                    subfiles = {}
                    for subfile in stream.text.split('\n'):
                        urlparts[txidurlidx] = filetxids[subfile]
                        subfiles[subfile] = '/'.join(urlparts)
                else:
                    subfiles = {}
                    subfilenames = stream.text.split('\n')
                    stream.close()
                    for subfile in subfilenames:
                        subfiles[subfile] = url + '/' + subfile
                def get_stream(url):
                    return requests.get(url, stream=True)
                with open(local_file, 'wb') as file:
                    for response in concurrent.futures.ThreadPoolExecutor(max_workers=len(subfiles)).map(requests.get, subfiles.values()):
                        url = response.url
                        self._debug(url)
                        response = requests.get(url)
                        response.raise_for_status()
                        urls = []
                        for txid in response.text.split('\n'):
                            urlparts[txidurlidx] = txid
                            url = '/'.join(urlparts)
                            urls.append(url)
                        for inner_response in concurrent.futures.ThreadPoolExecutor(max_workers=5).map(get_stream, urls):
                            inner_response.raise_for_status()
                            url = inner_response.url
                            self._debug(url)
                            #with requests.get(url, stream=True) as stream:
                            with inner_response as stream:
                                for chunk in stream.iter_content(chunk_size=1024*1024):
                                    file.write(chunk)
                                    size += len(chunk)
                                    self.annex.progress(size)
                        #self.annex.progress(size)
            else:
                with open(local_file, 'wb') as file:
                    for chunk in stream.iter_content(chunk_size=65536):
                        file.write(chunk)
                        size += len(chunk)
                        self.annex.progress(size)

    def remove(self, key):
        if self.combine_to:
            self.combined -= self.combining.pop(key, 0)

        for txid in self.txids(key):
            for url in self.txid_urls(txid):
                self.annex.seturlmissing(key, url)
            for uri in self.txid_uris(txid):
                self.annex.seturimissing(key, uri)
            self.annex.seturimissing(key, URI_PROTO + txid)

    def txids(self, key):
        return [url.split('/',3)[-1] for url in (*self.annex.geturls(key, self.gateway + '/'), *self.annex.geturls(key, URI_PROTO))]

    def _txid_request(self, method, key, txid, subdir=''):
        for url in self.txid_urls(txid):
            try:
                response = requests.request(method=method, url=url+subdir)
                response.raise_for_status()
                return response
            except Exception as exc:
                last_exc = exc
                continue
        for url in self.txid_tmpurls(txid):
            try:
                response = requests.request(method=method, url=url+subdir)
                response.raise_for_status()
                self._info(f'{txid} stalled at {url}')
                self._info(f'{key} {txid} uploaded at {self.last_key_date(key)}')
                return response
            except Exception as exc:
                last_exc = exc
                continue
        self._info('_txid_request ' + key + ' ' + txid + ': ' + str(last_exc))
        raise RemoteError('_txid_request ' + key + ' ' + txid + ': ' + str(last_exc))

    def checkpresent(self, key):
        if self.combine_to and key in self.combining:
            return True
        last_exc = None
        for txid in self.txids(key):
            try:
                response = self._txid_request('HEAD', key, txid)
                if response.headers['Content-Type'] == 'application/x.arweave-manifest+json' or response.url[-1] == '/':
                    '''this is a split file, ensure all the parts exist'''
                    if response.headers['Content-Type'] == 'application/x.arweave-manifest+json':
                        manifest = self._txid_request('GET', key, txid).json()
                        filetxids = {key: value['id'] for key, value in manifest['paths']}
                        index_file = manifest['index']
                        response = self._txid_request('GET', key, filetxids[index_file])
                    subfiles = {}
                    for subfile in response.text.split('\n'):
                        subfiles[subfile] = filetxids[subfile]
                    # note: subfiles has only the txid, not the whole url
                    def check_subtx(txid):
                        return self._txid_request('HEAD', key, txid)
                    def get_subtx(txid):
                        return self._txid_request('GET', key, txid)
                    for response in concurrent.futures.ThreadPoolExecutor(max_workers=len(subfiles)).map(get_subtx, subfiles.values()):
                        txids = response.text.split('\n')
                        for inner_response in concurrent.futures.ThreadPoolExecutor(max_workers=5).map(check_subtx, txids):
                            pass
                return True
            except exc:
                last_exc = exc
                continue
        if last_exc.response.status_code == 404:
            self._info(f'remove and readd {key} from this remote to hide this failure')
        return False

    def getavailability(self):
        return 'global'

    def txid_tmpurls(self, txid):
        if self.bundler:
            return [
                f'{self.bundler}/tx/{txid}/data',
            ]
        else:
            return []


    def txid_urls(self, txid):
        return [
            f'{self.gateway}/{txid}',
        ]

    def txid_uris(self, txid):
        return [
        ]

    #def whereis(self, key):
    #    item = self.stored.get(key)
    #    if item is None:
    #        return ''
    #    else:
    #        cid = item['cid']
    #        return ' '.join((self.cid_uris(cid)))

    def arkb(self, *params, input=None, backoff_secs=8, retries=10):
        try:
            self._debug('arkb ' + ' '.join((str(param) for param in params)))
            cmd = [*os.environ.get('ARKB', 'arkb').split(' '), *params]
            result = subprocess.run(
                cmd,
                env={**os.environ, 'HOME':self.local_dir},
                input=input,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                check=True,
                text=True
            )
            stdout = ansi_escape.sub('', result.stdout)
            if 'Failed to deploy' in stdout:
                raise subprocess.CalledProcessError(0, cmd[0], stdout=result.stdout, stderr=result.stderr)
            stderr = ansi_escape.sub('', result.stderr)
            for line in stdout.split('\n'):
                self._debug(line[:80])
            for line in stderr.split('\n'):
                self._debug(line[:80])
            self._debug('PROCESS SUCCESS arkb ' + ' '.join(params))
        except subprocess.CalledProcessError as err:
            #self.confirmed = 0 # don't clean up state
            self._info('err: arkb ' + ' '.join(params) + repr(err))
            stdout = ansi_escape.sub('', err.stdout)
            stderr = ansi_escape.sub('', err.stderr)
            for line in stdout.split('\n'):
                self._info(line[:80])
            for line in stderr.split('\n'):
                self._info(line[:80])
            if retries and 'toJSON' in (stdout + stderr):# or 'Too Many Requests' in err.stdout:
                delay = backoff_secs + backoff_secs * random.random()
                self._info(f'Waiting {int(delay+0.5)} seconds.')
                for line in err.stdout.split('\n'):
                    self._info(line)
                for line in err.stderr.split('\n'):
                    self._info(line)
                time.sleep(delay)
                self._info(f'Done waiting for {int(delay+0.5)} seconds.')
                return self.arkb(*params, input=input, backoff_secs = delay, retries=retries-1)
            elif 'EAI_AGAIN' in (stdout + stderr):
                self._info(f'EAI_AGAIN')
                return self.w3(*params, input=input, backoff_secs = backoff_secs,retries=retries)
            elif retries:
                return self.arkb(*params, input=input, backoff_secs=backoff_secs, retries=retries-1)
            raise RemoteError((err.stdout + ' '  + err.stderr).replace('\n', ' '))
        return stdout
    #def setup(self):
    #    subprocess.run(('w3', 

    #def keystatuses(self, key):
    #    item = self.stored.get(key, dict(pin=[], deal=[]))
    #    result = set((*(pin['status'] for pin in item.get('pins',())), *(deal['status'] for deal in item.get('deals',()))))
    #    if '_git_annex_just_put' in item:
    #        result.add('JustPut')
    #    return result

    def last_key_date(self, key):
        import subprocess
        gitdir = self.annex.getgitdir()
        proc = subprocess.Popen(
            ('git', '--git-dir', gitdir, 'annex', 'log', '--all'),
            stdout=subprocess.PIPE,
            text=True
        )
        try:
            for line in proc.stdout:
                logdata, remotedata = line.split(' | ', 1)
                remote_uuid, remote_name_unparsed = remotedata.split(' -- ', 1)
                if remote_uuid != self.uuid:
                    continue
                logdata, logkey = logdata.rsplit(' ', 1)
                if logkey != key:
                    continue
                state, date = logdata.split(' ', 1)
                if state == '+':
                    return date
        finally:
            proc.terminate()
        return None

if __name__ == '__main__':
    main = Main()
    remote = ArkbStorageRemote(main)
    main.LinkRemote(remote)
    try:
        main.Listen()
    finally:
        remote.finish()
