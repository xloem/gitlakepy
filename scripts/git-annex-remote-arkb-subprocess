#!/usr/bin/env python3

from annexremote import Master as Main, SpecialRemote, RemoteError, UnsupportedRequest

import ar, bundlr

import concurrent.futures
import datetime
import json, os, random, resource, shutil, subprocess, sys, time
import re, requests

ansi_escape = re.compile(r'\x1B(?:[@-Z\\-_]|\[[0-?]*[ -/]*[@-~])')

TXID_LEN = 43
URI_PROTO = 'arkb-subprocess://'

try:
    max_files, _ = resource.getrlimit(resource.RLIMIT_NOFILE)
except ValueError:
    max_files = 1024

class ArkbStorageRemote(SpecialRemote):
    def __init__(self, annex):
        super().__init__(annex)
        self.configs = {
            'wallet': 'Path or jwk wallet',
            'bundler': 'ANS-104 bundler node url',
            'gateway': 'Arweave gateway url',
            'combine-to-bytes': 'Consolidate uploads up to this many bytes',
            'subchunk-bytes': 'Break uploads into subfiles of this many bytes',
            'timeout': 'Network timeout in seconds',
        }
        self.local_dir = None
        self.closed = False

    def initremote(self):
		# initialize in repo, e.g. create folders or change settings
        # 'git annex initremote' / 'git annex enableremote'

        wallet = self.annex.getconfig('wallet')
        #token = self.annex.getcreds('token')['password']
        if wallet == '':
            self._info('Warning: no wallet specified. Only subsidised uploads will work.')
        else:
            try:
                with open(wallet, 'rt') as wf:
                    wallet = wf.read()
                self.annex.setconfig('wallet', wallet.replace('\n', ' '))
            except:
                pass

        #self.w3('token', input=token+'\n')
        self.prepare()


    def prepare(self):
        # prepare to be used for transfers, e.g. open connection
        self.uuid = self.annex.getuuid()
        self.git_dir = self.annex.getgitdir()
        self.local_dir = os.path.join(self.git_dir, self.__class__.__name__, self.uuid)
        os.makedirs(self.local_dir, exist_ok=True)
        if self.annex.getconfig('wallet') != '':
            self.wallet_path = os.path.join(self.local_dir, 'wallet.json')
            try:
                with open(self.wallet_path, 'rt') as wallet_file:
                    wallet = wallet_file.read()
                assert wallet == self.annex.getconfig('wallet')
            except:
                tmp_wallet_path = self.wallet_path + '.tmp'
                with open(tmp_wallet_path, 'wt') as wallet_file:
                    wallet_file.write(self.annex.getconfig('wallet'))
                os.chmod(tmp_wallet_path, 0o400)
                os.rename(tmp_wallet_path, self.wallet_path)
            self.params_deploy = ('--wallet', self.wallet_path, '--no-colors')
        else:
            self.wallet_path = ''
            self.params_deploy = ('--no-colors',)

        self._info('arkb ' + self.arkb('version').strip())

        self.ar_gateways = [ar.Peer(outgoing_connections = min(max_files // 4, ar.DEFAULT_REQUESTS_PER_MINUTE_LIMIT))]
        self.ar_peers = self.ar_gateways

        self.bundler = self.annex.getconfig('bundler')
        if self.bundler:
            self.params_deploy = (*self.params_deploy, '--use-bundler', self.bundler)
            self.bundlr_nodes = [bundlr.Node(self.bundler)]
        else:
            self.bundlr_nodes = []
        self.gateway = self.annex.getconfig('gateway')
        if not self.gateway:
            self.gateway = 'https://arweave.net'
        else:
            self.params_deploy = (*self.params_deploy,'--gateway', self.gateway)
        self.timeout = self.annex.getconfig('timeout')
        if not self.timeout:
            self.timeout = str(2147483646 / 1000)
        self.params_deploy = (*self.params_deploy,'--timeout', str(int(float(self.timeout) * 1000)))

        self.combine_to = self.annex.getconfig('combine-to-bytes')
        if not self.combine_to:
            #if self.bundler:
            #    self.combine_to = 100000 * 32
            #else:
                self.combine_to = 0
        self.combining = {}
        self.combined = 0
        self.combined_pathnames = 0
        if self.combine_to:
            self.combine_to = int(self.combine_to)
        self.subchunk = self.annex.getconfig('subchunk-bytes')
        if self.subchunk != '':
            self.subchunk = int(self.subchunk)
        self.urlqueue_dir = os.path.join(self.local_dir, 'urlqueue')
        try:
            for file in os.listdir(self.urlqueue_dir):
                with open(os.path.join(self.urlqueue_dir, file), 'rt') as fh:
                    for line in fh:
                        key, url = line.rstrip().split(' ', 1)
                        self.annex.seturlpresent(key, url)
                os.unlink(file)
        except FileNotFoundError:
            pass
        if self.combine_to or self.subchunk:
            if self.combine_to and self.subchunk and self.subchunk > self.combine_to:
                raise RemoteError('unimplemented: subchunk-bytes > combine-to-bytes')
                
            self.combining_dir = os.path.join(self.local_dir, str(os.getpid()))
            os.makedirs(self.combining_dir, exist_ok = True)

    def do_combine(self, tags_dict = {}, **tags):
        if len(self.combining) == 0:
            return
        index_filename = 'index.txt'
        with open(os.path.join(self.combining_dir, index_filename), 'wt') as index_file:
            index_file.write('\n'.join(self.combining.keys()))
        if tags:
            tags_dict = tags_dict.copy()
            tags_dict.update(tags)
        keytagparams = []
        for name, value in tags_dict.items():
            keytagparams.extend((
                '--tag-name', name,
                '--tag-value', value
            ))
        output = self.arkb(
            'deploy',
            self.combining_dir,
            *keytagparams,
            '--concurrency', str(5 if len(self.combining) < 5 else min(len(self.combining), max_files//4)),
            '--auto-confirm',
            '--index', index_filename,
            *self.params_deploy
        )
        manifest_filename = os.path.join(self.combining_dir, 'manifest.arkb')
        if not os.path.exists(manifest_filename):
            for path in self.combining.keys():
                manifest_filename = os.path.join(self.combining_dir, os.path.dirname(path), 'manifest.arkb')
                if os.path.exists(manifest_filename):
                    break
        if not os.path.exists(manifest_filename):
            self.combined = 0 # to quickly prevent cleanup during debugging
            self.combined_pathnames = 0
            self.combining_dir = self.combining_dir + '_'
            raise RemoteError(f'not found on fs after arkb deploy: {manifest_filename}')
        lines = output.split('\n')
        self._debug('lines[-3] = ' + repr(lines[-3]))
        self._debug('lines[-2] = ' + repr(lines[-2]))
        self._debug('lines[-1] = ' + repr(lines[-1]))
        manifest_txid = lines[-2].split('/')[-1]
        with open(manifest_filename) as manifest_file:
            manifest = json.load(manifest_file)
        key_urls = []
        file_txids = {}
        for key, props in manifest['paths'].items():
            txid = props['id']
            file_txids[key] = txid
            self._debug(f'{key}: {self.gateway}/{txid}')
            if self.combining.get(key, False):
                for url in (*self.txid_urls(txid), *self.txid_uris(txid)):
                    key_urls.append((key, url))
        if self.closed:
            example_key_url = key_urls[0][0]
            key_urls = '\n'.join((' '.join(keyurl) for keyurl in key_urls))
            urlqueue_fn = str(os.getpid()) + '.urls'
            with open(os.path.join(self.local_dir, urlqueue_fn), 'wt') as fh:
                fh.write(key_urls)
            os.rename(os.path.join(self.local_dir, urlqueue_fn), os.path.join(self.urlqueue_dir, urlqueue_fn))
            result = subprocess.run(
                ('git', 'annex', '--debug', 'checkpresentkey', example_key_url, self.uuid),
                input=keyurls,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                check=True,
                text=True
            )
            for line in result.stdout:
                self._debug(line)
            for line in result.stderr:
                self._info(line)
        else:
            for key, url in key_urls:
                self.annex.seturlpresent(key, url)
        shutil.rmtree(self.combining_dir)
        os.makedirs(self.combining_dir)
        self.combined = 0
        self.combined_pathnames = 0
        self.combining = {}
        return manifest_txid, file_txids

    def finish(self):
        self.closed = True
        self.tty = open('/dev/tty', 'wt', encoding='utf-8')
        if (hasattr(self, 'combine_to') and self.combine_to):
            if self.combined and any(self.combining.values()):
                self.do_combine()
        if (hasattr(self, 'combine_to') and self.combine_to) or (hasattr(self, 'subchunk') and self.subchunk):
            shutil.rmtree(self.combining_dir)
    def _info(self, txt):
        if self.closed:
            self.tty.write(f'Pre-shutdown: {txt}\n')
        else:
            self.annex.info(txt)
    def _debug(self, txt):
        if self.closed:
            return self._info(txt)
        else:
            self.annex.debug(txt)

    def transfer_store(self, key, filename):
        with open(filename, 'rb') as file:
            file.seek(0, os.SEEK_END)
            size = file.tell()
        if self.subchunk and size > self.subchunk:
            new_subdir = key
            new_fulldir = os.path.join(self.combining_dir, new_subdir); os.makedirs(new_fulldir, exist_ok=True)
            progress = 0
            with open(filename, 'rb') as file:
                keytxids = {}
                for offset in range(0, size, self.subchunk):
                    newfilename = str(offset)
                    data = file.read(self.subchunk)
                    if (self.combine_to and self.combined + len(data) > self.combine_to) or (self.combined_pathnames + len(newfilename)) * 56 + 77 + len('index.txt') > self.subchunk:
                        manifest_txid, file_txids = self.do_combine()
                        keytxids.update(file_txids)
                        self.annex.progress(progress)
                        new_fulldir = os.path.join(self.combining_dir, new_subdir); os.makedirs(new_fulldir, exist_ok=True)
                    progress += len(data)
                    self.combined += len(data)
                    self.combined_pathnames += len(os.path.join(new_subdir, newfilename))
                    self.combining[new_subdir + '/' + newfilename] = False
                    with open(os.path.join(new_fulldir, newfilename), 'wb') as newfile:
                        newfile.write(data)
            keytxids.update(self.do_combine()[1])
            txids = []
            self._debug('keytxids: ' + repr(keytxids))
            keytxids = [(*key.split('/',1), txid) for key, txid in keytxids.items() if '/' in key]
            keytxids.sort(key=lambda item: int(item[1]))
            txids = [txid for txkey, offset, txid in keytxids if txkey == key]
            assert len(txids) > 0
            
            # so we'll probably have multiple index files
            # in case there are many subchunks.
            file_idx = 0
            idx_file = None
            for txid in txids:
                if idx_file is None or idx_file_size + len(txid) + 1 > self.subchunk:
                    if idx_file is not None:
                        self.combined += idx_file_size
                        idx_file.close()
                    file_idx += 1
                    idx_file = open(os.path.join(self.combining_dir, str(file_idx)), 'wt')
                    self.combined_pathnames += len(str(file_idx))
                    self.combining[str(file_idx)] = False
                    idx_file.write(txid)
                    idx_file_size = len(txid)
                else:
                    idx_file.write('\n' + txid)
                    idx_file_size += 1 + len(txid)
            self.combined += idx_file_size
            idx_file.close()

            manifest_txid, file_txids = self.do_combine({
                'application': 'git-annex-remote-arkb-subprocess',
                'format': 'subchunk-txid-files',
                'git-annex-key': key
            })

            self.annex.seturipresent(key, URI_PROTO + manifest_txid)

            self.annex.progress(progress)
        elif self.combine_to and size < self.combine_to:
            if self.combined + size > self.combine_to:
                self.do_combine()
            newfilename = key #'c-' + str(len(self.combining))
            newfullpath = os.path.join(self.combining_dir, newfilename)
            try:
                os.rename(filename, newfullpath)
            except:
                shutil.copyfile(filename, newfullpath)
            self.combining[key] = True#size#newfullpath#(newfilename, keytagparams)
            self.combined += size
            self.combined_pathnames += len(newfilename)
        else:
            keybackend, keyname = key.split('--')
            keybackend, *keyparts = keybackend.split('-')
            keyparts = {
                keypart[0]: keypart[1:]
                for keypart in keyparts
            }
            keyparts['backend'] = keybackend
            keyparts['name'] = keyname
            keytagparams = []
            for name, value in keyparts.items():
                keytagparams.extend((
                    '--tag-name', 'git-annex-key-' + name,
                    '--tag-value', value
                ))
            keytagparams.extend((
                    '--tag-name', 'IPFS-Add',
                    '--tag-value', key,
                    '--tag-name', 'git-annex-key',
                    '--tag-value', key,
            ))
            output = self.arkb(
                    'deploy',
                    filename,
                    *keytagparams,
                    '--auto-confirm',
                    *self.params_deploy
            )
            if 'Failed to deploy' in output:
                raise RemoteError(output.replace('\n', ' '))
            lines = output.split('\n')
            self._debug('lines[-3] = ' + repr(lines[-3]))
            self._debug('lines[-2] = ' + repr(lines[-2]))
            self._debug('lines[-1] = ' + repr(lines[-1]))
            txid = lines[-2].split('/')[-1]
            for url in self.txid_urls(txid):
                self.annex.seturlpresent(key, url)
            for uri in self.txid_uris(txid):
                self.annex.seturipresent(key, uri)

    def transfer_retrieve(self, key, local_file):
        if self.combine_to and key in self.combining:
            self.do_combine()
        last_exc = RemoteError('Logic error? No urls stored for key ' + key)
        for txid in self.txids(key):
            txid_urls = self.txid_urls(txid)
            for url in txid_urls:
                try:
                    self.retrieve_url(url, local_file)
                    return
                except Exception as exc:
                    last_exc = exc
                    continue
            txid_tmpurls = self.txid_tmpurls(txid)
            for url in txid_tmpurls:
                try:
                    self.retrieve_url(url, local_file)
                    self._info(f'{key} or a subpart stalled at {url}')
                    self._info(f'{key} uploaded at {self.key_urls_date(key, *txid_urls, *txid_tmpurls, ":" + URI_PROTO + txid)}')
                    return
                except Exception as exc:
                    last_exc = exc
                    continue
        try:
            raise last_exc
        except:
            ar.logger.error(f'{gateway.api_url}/{txid+subdir}: ' + str(last_exc))
        self._info('retrieve_url: ' + str(last_exc))
        raise RemoteError('retrieve_url: ' + str(last_exc))

    def retrieve_url(self, url, local_file):
        self._debug(f'retrieve_url({repr(url)}, {repr(local_file)}')
        urlparts = url.split('/')
        for idx, part in enumerate(urlparts):
            if len(part) == TXID_LEN:
                txidurlidx = idx
                break
        self._debug(f'txid = {urlparts[txidurlidx]}')

        size = 0

        with requests.get(url, stream=True) as stream:
            stream.raise_for_status()
            # manifest from bundler node indicating folder
            # or '/' trailing url from gateway indicating folder
            # assume subchunk format, files containing txids to be concatenated
            self._debug(f'Content-Type = {stream.headers["Content-Type"]}')
            self._debug(f'url = {stream.url}')
            if stream.headers['Content-Type'] == 'application/x.arweave-manifest+json' or stream.url[-1] == '/':
                if stream.headers['Content-Type'] == 'application/x.arweave-manifest+json':
                    manifest = stream.json()
                    stream.close()
                    assert manifest['manifest'] == 'arweave/paths'
                    assert manifest['version'] == '0.1.0'
                    filetxids = {key: value['id'] for key, value in manifest['paths'].items()}
                    index_file = manifest['index']['path']
                    urlparts[txidurlidx] = filetxids[index_file]
                    stream = requests.get('/'.join(urlparts))
                    stream.raise_for_status()
                    subfiles = {}
                    for subfile in stream.text.split('\n'):
                        urlparts[txidurlidx] = filetxids[subfile]
                        subfiles[subfile] = '/'.join(urlparts)
                else:
                    subfiles = {}
                    subfilenames = stream.text.split('\n')
                    stream.close()
                    for subfile in subfilenames:
                        subfiles[subfile] = url + '/' + subfile
                def get_stream(url):
                    return requests.get(url, stream=True)
                with open(local_file, 'wb') as file:
                    for response in concurrent.futures.ThreadPoolExecutor(max_workers=min(len(subfiles), max_files//4)).map(requests.get, subfiles.values()):
                        url = response.url
                        self._debug(url)
                        response = requests.get(url)
                        response.raise_for_status()
                        urls = []
                        for txid in response.text.split('\n'):
                            urlparts[txidurlidx] = txid
                            url = '/'.join(urlparts)
                            urls.append(url)
                        for inner_response in concurrent.futures.ThreadPoolExecutor(max_workers=5).map(get_stream, urls):
                            inner_response.raise_for_status()
                            url = inner_response.url
                            self._debug(url)
                            #with requests.get(url, stream=True) as stream:
                            with inner_response as stream:
                                for chunk in stream.iter_content(chunk_size=1024*1024):
                                    file.write(chunk)
                                    size += len(chunk)
                                    self.annex.progress(size)
                        #self.annex.progress(size)
            else:
                with open(local_file, 'wb') as file:
                    for chunk in stream.iter_content(chunk_size=65536):
                        file.write(chunk)
                        size += len(chunk)
                        self.annex.progress(size)

    def remove(self, key):
        if self.combine_to:
            self.combined -= self.combining.pop(key, 0)

        for txid in self.txids(key):
            for url in self.txid_urls(txid):
                self.annex.seturlmissing(key, url)
            for uri in self.txid_uris(txid):
                self.annex.seturimissing(key, uri)
            self.annex.seturimissing(key, URI_PROTO + txid)

    def txids(self, key):
        return [url.split('/',3)[-1] for url in (*self.annex.geturls(key, self.gateway + '/'), *self.annex.geturls(key, URI_PROTO))]

    def _txid_request(self, method, key, txid, subdir='', root_txid=None):
        #self._debug(f'{key} {method} {txid}')
        if subdir is None:
            subdir = ''
        txid_urls = self.txid_urls(txid)
        ## could use concurrent.futures.ThreadPoolExecutor somewhere here for speedup
        #for url in txid_urls:
        #    try:
        #        response = requests.request(method=method, url=url+subdir)
        #        response.raise_for_status()
        #        return response
        #    except Exception as exc:
        #        last_exc = exc
        #        continue
        for gateway in self.ar_gateways:
            try:
                response = gateway._request(txid + subdir, method=method)
                return response
            except Exception as exc:
                ar.logger.error(f'{gateway.api_url}/{txid+subdir}: ' + str(exc))
                self._info(f'{gateway.api_url}/{txid+subdir}: ' + str(exc))
                last_exc = exc
                continue
        if subdir:
            manifest = self._txid_request('GET', key, txid, root_txid=root_txid).json()
            if root_txid is None:
                root_txid = txid
            txid = manifest['paths'][subdir[1:]]
        txid_tmpurls = self.txid_tmpurls(txid)
        for node in self.bundlr_nodes:
            try:
                response = node._request('tx', txid, 'data', method=method)
                if root_txid is None or root_txid == txid:
                    root_txid = txid
                    root_urls = txid_urls + txid_tmpurls + [':' + URI_PROTO + root_txid]
                else:
                    root_urls = [':' + URI_PROTO + root_txid] #self.txid_urls(root_txid) + self.txid_tmpurls(root_txid)
                root_txid = root_txid or txid
                self._info(f'{txid} stalled at {node.api_url}/tx/{txid}/data')
                self._info(f'{key} {txid} uploaded at {self.key_urls_date(key, *root_urls)}')
                return response
            except Exception as exc:
                last_exc = exc
                continue
        #for url in txid_tmpurls:
        #    try:
        #        response = requests.request(method=method, url=url)
        #        response.raise_for_status()
        #        if root_txid is None or root_txid == txid:
        #            root_txid = txid
        #            root_urls = txid_urls + txid_tmpurls + [':' + URI_PROTO + root_txid]
        #        else:
        #            root_urls = [':' + URI_PROTO + root_txid] #self.txid_urls(root_txid) + self.txid_tmpurls(root_txid)
        #        root_txid = root_txid or txid
        #        self._info(f'{txid} stalled at {url}')
        #        self._info(f'{key} {txid} uploaded at {self.key_urls_date(key, *root_urls)}')
        #        return response
        #    except Exception as exc:
        #        last_exc = exc
        #        continue
        self._info('_txid_request ' + str(key) + ' ' + str(txid) + ': ' + str(last_exc))
        raise RemoteError('_txid_request ' + str(key) + ' ' + str(txid) + ': ' + str(last_exc))

    def checkpresent(self, key):
        if self.combine_to and key in self.combining:
            return True
        last_exc = None
        missing_txids = []
        for txid in self.txids(key):
            try:
                response = self._txid_request('HEAD', key, txid)
                self._debug(response.url + ': ' + response.headers.get('Content-Type', 'no Content-Type header'))
                if response.headers.get('Content-Type', None) == 'application/x.arweave-manifest+json' or response.url[-1] == '/':
                    '''this is a split file, ensure all the parts exist'''
                    subfiles = {}
                    if response.headers.get('Content-Type', None) == 'application/x.arweave-manifest+json':
                        manifest = self._txid_request('GET', key, txid).json()
                        filetxids = {key: value['id'] for key, value in manifest['paths'].items()}
                        index_file = manifest['index']['path']
                        response = self._txid_request('GET', key, filetxids[index_file], root_txid=txid)
                        for subfile in response.text.split('\n'):
                            subfiles[subfile] = (filetxids[subfile], None)
                    else:
                        response = self._txid_request('GET', key, txid)
                        for subfile in response.text.split('\n'):
                            subfiles[subfile] = (txid, '/' + subfile)
                    self._debug(str(subfiles))
                    # note: subfiles has only the txid, not the whole url
                    def check_subtx(subtxid):
                        return self._txid_request('HEAD', key, subtxid, root_txid=txid)
                    def get_subtx(params):
                        return self._txid_request('GET', key, *params, root_txid=txid)
                    for response in concurrent.futures.ThreadPoolExecutor(max_workers=min(len(subfiles), 4)).map(get_subtx, subfiles.values()):
                        txids = response.text.split('\n')
                        for inner_response in concurrent.futures.ThreadPoolExecutor(max_workers=min(len(txids), max_files//8)).map(check_subtx, txids):
                            pass
                for missing_txid in missing_txids:
                    # speeds things up in the future if missing txids that get iterated are removed
                    # this is only done if the item is found among a different one
                    for missing_url in self.txid_urls(missing_txid):
                        self.annex.seturlmissing(key, missing_url)
                    for missing_uri in self.txid_uris(missing_txid):
                        self.annex.seturimissing(key, missing_uri)
                    self.annex.seturimissing(key, URI_PROTO + missing_txid)
                return True
            except Exception as exc:
                missing_txids.append(txid)
                last_exc = exc
                continue
        if hasattr(last_exc, 'response') and last_exc.response.status_code == 404:
            # we could also just drop it, or git-annex might retain the failure, maybe if ownership is claimed of the url
            self._info(f'remove and readd {key} from this remote to hide this failure. data is reused if the same machine is used to reupload.')
        elif last_exc is not None:
            raise last_exc
        return False

    def getavailability(self):
        return 'global'

    def txid_tmpurls(self, txid):
        if self.bundler:
            return [
                f'{self.bundler}/tx/{txid}/data',
            ]
        else:
            return []


    def txid_urls(self, txid):
        return [
            f'{self.gateway}/{txid}',
        ]

    def txid_uris(self, txid):
        return [
        ]

    #def whereis(self, key):
    #    item = self.stored.get(key)
    #    if item is None:
    #        return ''
    #    else:
    #        cid = item['cid']
    #        return ' '.join((self.cid_uris(cid)))

    def arkb(self, *params, input=None, backoff_secs=8, retries=10):
        try:
            self._debug('arkb ' + ' '.join((str(param) for param in params)))
            cmd = [*os.environ.get('ARKB', 'arkb').split(' '), *params]
            result = subprocess.run(
                cmd,
                env={**os.environ, 'HOME':self.local_dir},
                input=input,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                check=True,
                text=True
            )
            stdout = ansi_escape.sub('', result.stdout)
            if 'Failed to deploy' in stdout:
                raise subprocess.CalledProcessError(0, cmd[0], output=result.stdout, stderr=result.stderr)
            stderr = ansi_escape.sub('', result.stderr)
            for line in stdout.split('\n'):
                self._debug(line[:80])
            for line in stderr.split('\n'):
                self._debug(line[:80])
            self._debug('PROCESS SUCCESS arkb ' + ' '.join(params))
        except subprocess.CalledProcessError as err:
            #self.confirmed = 0 # don't clean up state
            self._info('err: arkb ' + ' '.join(params) + repr(err))
            stdout = ansi_escape.sub('', err.stdout)
            stderr = ansi_escape.sub('', err.stderr)
            for line in stdout.split('\n'):
                self._info(line[:80])
            for line in stderr.split('\n'):
                self._info(line[:80])
            if retries and 'toJSON' in (stdout + stderr):# or 'Too Many Requests' in err.stdout:
                delay = backoff_secs + backoff_secs * random.random()
                self._info(f'Waiting {int(delay+0.5)} seconds.')
                for line in err.stdout.split('\n'):
                    self._info(line)
                for line in err.stderr.split('\n'):
                    self._info(line)
                time.sleep(delay)
                self._info(f'Done waiting for {int(delay+0.5)} seconds.')
                return self.arkb(*params, input=input, backoff_secs = delay, retries=retries-1)
            elif 'EAI_AGAIN' in (stdout + stderr):
                self._info(f'EAI_AGAIN')
                return self.w3(*params, input=input, backoff_secs = backoff_secs,retries=retries)
            elif retries:
                return self.arkb(*params, input=input, backoff_secs=backoff_secs, retries=retries-1)
            raise RemoteError((err.stdout + ' '  + err.stderr).replace('\n', ' '))
        return stdout
    #def setup(self):
    #    subprocess.run(('w3', 

    #def keystatuses(self, key):
    #    item = self.stored.get(key, dict(pin=[], deal=[]))
    #    result = set((*(pin['status'] for pin in item.get('pins',())), *(deal['status'] for deal in item.get('deals',()))))
    #    if '_git_annex_just_put' in item:
    #        result.add('JustPut')
    #    return result

    def key_urls_date(self, key, *urls):
        dirhash_lower = self.annex.dirhash_lower(key)
        self._debug(f'dirhash_lower(key) = ' + dirhash_lower)
        self._debug(f'gitdir = ' + self.git_dir)
        lsweburl = subprocess.run(
            ('git', '--git-dir', self.git_dir, 'ls-tree', '--full-tree', 'git-annex', os.path.join(dirhash_lower, key + '.log.web')),
            stdout=subprocess.PIPE,
            check=True,
            text=True
        ).stdout
        mode, objtype, objid = lsweburl.split('\t',1)[0].split(' ')
        proc = subprocess.Popen(
            ('git', 'cat-file', objtype, objid),
            stdout = subprocess.PIPE,
            text=True
        )
        timestamps = {}
        try:
            for line in proc.stdout:
                timestamp, state, url = line[:-1].split(' ', 3)
                if url.endswith('\n'):
                    url = url[:-1]
                #self._debug(url + ' ?in? ' + str(urls))
                if url in urls:
                    if timestamp.endswith('s'):
                        timestamp = timestamp[:-1]
                    timestamps[url] = float(timestamp)
                    if len(timestamps) >= len(urls):
                        break
        finally:
            proc.terminate()
        if len(timestamps):
            min_timestamp = min((timestamp for timestamp in timestamps.values()))
            return datetime.datetime.fromtimestamp(min_timestamp).isoformat()
        else:
            return f'![did not find any urls like {urls[0]} associated with {key}]!'

    def last_key_date(self, key):
        proc = subprocess.Popen(
            ('git', '--git-dir', self.git_dir, 'annex', 'log', '--all'),
            stdout=subprocess.PIPE,
            text=True
        )
        try:
            for line in proc.stdout:
                logdata, remotedata = line.split(' | ', 1)
                remote_uuid, remote_name_unparsed = remotedata.split(' -- ', 1)
                if remote_uuid != self.uuid:
                    continue
                logdata, logkey = logdata.rsplit(' ', 1)
                if logkey != key:
                    continue
                state, date = logdata.split(' ', 1)
                if state == '+':
                    return date
        finally:
            proc.terminate()
        return None

if __name__ == '__main__':
    main = Main()
    remote = ArkbStorageRemote(main)
    main.LinkRemote(remote)
    try:
        main.Listen()
    finally:
        remote.finish()
